---
title: 'Part 1: Multiple Linear Regression'
author: "Kerry Nixon"
date: "12/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(tidyverse)
library(palmerpenguins)
library(GGally)
library(stargazer)
```


```{r}
penguins %>% 
  ggpairs()
```


```{r}
penguins %>% 
  select(species, bill_length_mm:body_mass_g) %>% 
  ggpairs(aes(color = species))
```


## Build a few different models

```{r}
lm1 <- lm(body_mass_g ~ flipper_length_mm + species, data = penguins)
lm1

lm2 <- lm(body_mass_g ~ flipper_length_mm + species + sex, data = penguins)
lm2

lm3 <- lm(body_mass_g ~ flipper_length_mm + species + sex + bill_length_mm, data = penguins)
lm3

lm4 <- lm(body_mass_g ~ flipper_length_mm + species + sex + bill_length_mm + island, data = penguins)
lm4
```


## Find the AIC value for each model

```{r}
AIC(lm1)
AIC(lm2)
AIC(lm3) # Of these four model this model has the lowest AIC metric, indicating the best balance of model fit and complexity. 
AIC(lm4)
```


## Use the stargazer package for a table with multiple model outputs

```{r, results = 'asis'}

stargazer(lm1, lm3, lm4, type = "html")

```


## Omitted variable bias in action

In particular the Simpsons Paradox

```{r}

ggplot(data = penguins, aes(x = flipper_length_mm, y = bill_depth_mm)) + geom_point() + geom_smooth(method = "lm")
```


```{r}
ggplot(data = penguins, aes(x = flipper_length_mm, 
                            y = bill_depth_mm, 
                            group = species)) + 
  geom_point(aes(color = species)) + 
  geom_smooth(method = "lm")
```


```{r}

ggplot(data = penguins, aes(x = flipper_length_mm, y = bill_depth_mm)) + geom_point(aes(color = species)) + geom_smooth(method = "lm")
```

